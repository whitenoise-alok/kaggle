{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a11d08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import json\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import multiprocessing\n",
    "import zipfile\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from shutil import rmtree\n",
    "import time\n",
    "\n",
    "\n",
    "base_loc = \"/data/\"\n",
    "raw_zip_file = \"s3_data/kaggle_coleridge/raw/coleridgeinitiative-show-us-the-data.zip\"\n",
    "unzip_loc = \"s3_data/kaggle_coleridge/processed/unzipped/\"\n",
    "\n",
    "test_folder = \"s3_data/kaggle_coleridge/processed/unzipped/test/\"\n",
    "train_folder = \"s3_data/kaggle_coleridge/processed/unzipped/train/\"\n",
    "\n",
    "train_y_file = \"train.csv\"\n",
    "submission_file = \"sample_submission.csv\"\n",
    "\n",
    "output_folder = \"s3_data/kaggle_coleridge/processed/processing_1/\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7058d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(base_loc, unzip_loc)):\n",
    "    os.makedirs(os.path.join(base_loc, unzip_loc))\n",
    "else:\n",
    "    rmtree(os.path.join(base_loc, unzip_loc))\n",
    "    os.makedirs(os.path.join(base_loc, unzip_loc))\n",
    "    \n",
    "\n",
    "with zipfile.ZipFile(os.path.join(base_loc, raw_zip_file), 'r') as zip_ref:\n",
    "    zip_ref.extractall(os.path.join(base_loc, unzip_loc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8826ff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def jaccard(str1, str2): \n",
    "#     a = set(str1.lower().split()) \n",
    "#     b = set(str2.lower().split())\n",
    "#     c = a.intersection(b)\n",
    "#     return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "\n",
    "# def clean_text(txt):\n",
    "#     return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b000d267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_label_in_text(sent_tokens, label_tokens):\n",
    "    max_loc = len(sent_tokens) - len(label_tokens)\n",
    "    for i in range(0, max_loc):\n",
    "        temp_tokens = sent_tokens[i:i+len(label_tokens)]\n",
    "        \n",
    "        if temp_tokens == label_tokens:\n",
    "            return [i,  i-1+len(label_tokens)]\n",
    "    return []\n",
    "\n",
    "\n",
    "def get_tokens_loc(sent, sent_tokens):\n",
    "    tokens_loc_start, tokens_loc_end = [], []\n",
    "    \n",
    "    start_search = 0\n",
    "    for token in sent_tokens:\n",
    "        temp_loc = sent.find(token, start_search)\n",
    "        \n",
    "        assert temp_loc != -1, \"token <%s> not found in sentence <%s>\"%(token, sent)\n",
    "        tokens_loc_start.append(temp_loc)\n",
    "        tokens_loc_end.append(temp_loc + len(token))\n",
    "        \n",
    "        start_search += len(token)\n",
    "        \n",
    "    return tokens_loc_start, tokens_loc_end\n",
    "    \n",
    "    \n",
    "def get_sentences(ind, base_loc, train_folder, file_id, pub_title,dataset_title, dataset_label):\n",
    "\n",
    "    \n",
    "#     print(\"processing %s started\"%ind)\n",
    "    temp_file_loc = os.path.join(base_loc, train_folder, file_id+\".json\")\n",
    "    json_data = json.load(open(temp_file_loc, \"r\"))\n",
    "    sentences = []\n",
    "    \n",
    "    label_doc = nlp(dataset_label)\n",
    "    label_tokens = [str(token) for token in label_doc]\n",
    "    \n",
    "    for elem in json_data:\n",
    "        temp_text = elem[\"text\"]\n",
    "        temp_doc = nlp(temp_text)\n",
    "        sents = list(temp_doc.sents)\n",
    "        for s_ind, sent in enumerate(sents):\n",
    "            sent = str(sent)\n",
    "            \n",
    "            sent_doc = nlp(sent)\n",
    "            sent_tokens = [str(token) for token in sent_doc]\n",
    "            sent_tokens_start, sent_tokens_end = get_tokens_loc(sent, sent_tokens)\n",
    "            \n",
    "            locations = find_label_in_text(sent_tokens, label_tokens)\n",
    "            \n",
    "            if locations:\n",
    "                sentences.append({\n",
    "                    \"file\": file_id+ \".json\",\n",
    "                    \"publication\": pub_title,\n",
    "                    \"section\": elem[\"section_title\"],\n",
    "                    \"sentence_id\": s_ind,\n",
    "                    \"sentence\": sent,\n",
    "                    \"tokens\": sent_tokens,\n",
    "                    \"dataset_title\": dataset_title,\n",
    "                    \"label\": dataset_label,\n",
    "                    \"label_start_tokens\": locations[0],\n",
    "                    \"bael_end_tokens\": locations[1],\n",
    "                    \"label_start\": sent_tokens_start[locations[0]],\n",
    "                    \"label_end\": sent_tokens_end[locations[1]]\n",
    "                    \n",
    "                })\n",
    "            else:\n",
    "                sentences.append({\n",
    "                    \"file\": file_id+ \".json\",\n",
    "                    \"publication\": pub_title,\n",
    "                    \"section\": elem[\"section_title\"],\n",
    "                    \"sentence_id\": s_ind,\n",
    "                    \"sentence\": sent,\n",
    "                    \"tokens\": sent_tokens,\n",
    "                })\n",
    "#     print(\"Processing %s finished\"%ind)\n",
    "    return sentences\n",
    "def get_sentences_process(ind, base_loc, train_folder, file_id, pub_title,dataset_title, dataset_label, q):\n",
    "\n",
    "    \n",
    "#     print(\"processing %s started\"%ind)\n",
    "    temp_file_loc = os.path.join(base_loc, train_folder, file_id+\".json\")\n",
    "    json_data = json.load(open(temp_file_loc, \"r\"))\n",
    "    sentences = []\n",
    "    \n",
    "    label_doc = nlp(dataset_label)\n",
    "    label_tokens = [str(token) for token in label_doc]\n",
    "    \n",
    "    for elem in json_data:\n",
    "        temp_text = elem[\"text\"]\n",
    "        temp_doc = nlp(temp_text)\n",
    "        sents = list(temp_doc.sents)\n",
    "        for s_ind, sent in enumerate(sents):\n",
    "            sent = str(sent)\n",
    "            \n",
    "            sent_doc = nlp(sent)\n",
    "            sent_tokens = [str(token) for token in sent_doc]\n",
    "            sent_tokens_start, sent_tokens_end = get_tokens_loc(sent, sent_tokens)\n",
    "            \n",
    "            locations = find_label_in_text(sent_tokens, label_tokens)\n",
    "            \n",
    "            if locations:\n",
    "                sentences.append({\n",
    "                    \"file\": file_id+ \".json\",\n",
    "                    \"publication\": pub_title,\n",
    "                    \"section\": elem[\"section_title\"],\n",
    "                    \"sentence_id\": s_ind,\n",
    "                    \"sentence\": sent,\n",
    "                    \"tokens\": sent_tokens,\n",
    "                    \"dataset_title\": dataset_title,\n",
    "                    \"label\": dataset_label,\n",
    "                    \"label_start_tokens\": locations[0],\n",
    "                    \"bael_end_tokens\": locations[1],\n",
    "                    \"label_start\": sent_tokens_start[locations[0]],\n",
    "                    \"label_end\": sent_tokens_end[locations[1]]\n",
    "                    \n",
    "                })\n",
    "            else:\n",
    "                sentences.append({\n",
    "                    \"file\": file_id+ \".json\",\n",
    "                    \"publication\": pub_title,\n",
    "                    \"section\": elem[\"section_title\"],\n",
    "                    \"sentence_id\": s_ind,\n",
    "                    \"sentence\": sent,\n",
    "                    \"tokens\": sent_tokens,\n",
    "                })\n",
    "#     print(\"Processing %s finished\"%ind)\n",
    "    return q.put(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "926d5e88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19661\n",
      "(0, '/data/', 's3_data/kaggle_coleridge/processed/unzipped/train/', 'd0fa7568-7d8e-4db9-870f-f9c6f668c17b', 'The Impact of Dual Enrollment on College Degree Attainment: Do Low-SES Students Benefit?', 'National Education Longitudinal Study', 'National Education Longitudinal Study')\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(os.path.join(base_loc, unzip_loc,  train_y_file))\n",
    "inputs = []\n",
    "count = 0\n",
    "for i in range(train_df.shape[0]):\n",
    "    inputs.append((\n",
    "        count,\n",
    "        base_loc,\n",
    "        train_folder,\n",
    "        train_df.loc[i, \"Id\"],\n",
    "        train_df.loc[i, \"pub_title\"],\n",
    "        train_df.loc[i, \"dataset_title\"],\n",
    "        train_df.loc[i, \"dataset_label\"]\n",
    "    ))\n",
    "    count += 1\n",
    "\n",
    "print(len(inputs))\n",
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5d06ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing started\n",
      "1\n",
      "2 10\n",
      "closing 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-15:\n",
      "Process Process-17:\n",
      "Process Process-12:\n",
      "Process Process-11:\n",
      "Process Process-13:\n",
      "Process Process-19:\n",
      "Traceback (most recent call last):\n",
      "Process Process-18:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 261, in _bootstrap\n",
      "    util._exit_function()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 261, in _bootstrap\n",
      "    util._exit_function()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 261, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 261, in _bootstrap\n",
      "    util._exit_function()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 261, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 261, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 322, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 322, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 261, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 322, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 322, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 322, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 262, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 322, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 322, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 186, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 262, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 262, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 262, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 262, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 262, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 186, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 186, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 191, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 186, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 191, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 186, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 1056, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 191, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 262, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 191, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 1056, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 1056, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 191, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 1056, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 1056, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 186, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 186, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 191, in _finalize_join\n",
      "    thread.join()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 1056, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 191, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b5fb3bd3f8ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"closing\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/process.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWNOHANG\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0;31m# Child process not yet created. See #1731717\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 1056, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "inputs= inputs[:10]\n",
    "\n",
    "print(\"processing started\")\n",
    "st = round(time.time(), 0)\n",
    "\n",
    "# ORDINARY LOOP\n",
    "# output = [get_sentences(*i) for i in inputs]\n",
    "\n",
    "\n",
    "# POOL\n",
    "# process_pool = multiprocessing.Pool(3)\n",
    "# output = process_pool.starmap(get_sentences, inputs)\n",
    "\n",
    "\n",
    "# # PROCESS\n",
    "pools = []\n",
    "q = multiprocessing.Queue()\n",
    "print(1)\n",
    "for inp in inputs:\n",
    "    temp_inp = (inp[0], inp[1], inp[2], inp[3], inp[4], inp[5], inp[6], q)\n",
    "    p = multiprocessing.Process(target=get_sentences_process, args=temp_inp)\n",
    "    \n",
    "    p.start()\n",
    "    pools.append(p)\n",
    "\n",
    "output = []\n",
    "print(2, len(pools))\n",
    "for p_ind, p in enumerate(pools):\n",
    "    output.append(q.get())\n",
    "    print(\"closing\", p_ind)\n",
    "    p.join()\n",
    "    print(\"closed\", p_ind)\n",
    "\n",
    "\n",
    "en = round(time.time(), 0)\n",
    "print(\"program completed in %s seconds\"%(en-st))\n",
    "\n",
    "print(len(output))\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "007ba20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "22853\n"
     ]
    }
   ],
   "source": [
    "print(len(output))\n",
    "print(sum([len(o) for o in output]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cae71ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2461475\n",
      "37667\n",
      "2423808\n"
     ]
    }
   ],
   "source": [
    "print(len(output))\n",
    "positive_sentences = [o for o in output if \"label\" in o]\n",
    "negative_sentences = [o for o in output if \"label\" not in o]\n",
    "print(len(positive_sentences))\n",
    "print(len(negative_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "355a41c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_output_file = os.path.join(base_loc, output_folder, \"pos.json\")\n",
    "negative_output_file = os.path.join(base_loc, output_folder, \"neg.json\")\n",
    "\n",
    "json.dump(positive_sentences, open(positive_output_file, 'w'))\n",
    "json.dump(negative_sentences, open(negative_output_file, 'w'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
